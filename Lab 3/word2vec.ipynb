{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pretrained Word2vec model (Google news). Choose a short English text (about 400-500 words). For example you can take a wikipedia article or book excerpt. The text must also contain proper nouns. Solve the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Print the number of words in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Born into an upper-middle-class family, Van Gogh drew as a child and was serious, quiet and thoughtful, but showed signs of mental instability. As a young man, he worked as an art dealer, often travelling, but became depressed after he was transferred to London. He turned to religion and spent time as a missionary in southern Belgium. Later he drifted into ill-health and solitude. He was keenly aware of modernist trends in art and, while back with his parents, took up painting in 1881. His younger brother, Theo, supported him financially, and the two of them maintained a long correspondence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the model's vocabulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "vocab = set(model.key_to_index)\n",
    "print(\"Number of words in the model's vocabulary: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Print all the words in the text that do not appear in the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n",
      "travelling\n",
      "to\n",
      ".\n",
      "and\n",
      "upper-middle-class\n",
      "a\n",
      "of\n",
      ",\n",
      "ill-health\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "words = set(word_tokenize(text))\n",
    "\n",
    "for word in words:\n",
    "    if word not in vocab:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Which are the two most distant words in the text, and which are the closest? Print the distance too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most distant words are ('Born', 'them') with a distance of 0.6712614893913269\n",
      "The closest words are ('He', 'he') with a distance of -0.15342943370342255\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "word_pairs = list(combinations(words, 2))\n",
    "\n",
    "most_distant_words = None\n",
    "most_distant_words = None\n",
    "max_distance = -np.inf\n",
    "min_distance = np.inf\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    if word1 in vocab and word2 in vocab:\n",
    "        distance = model.similarity(word1, word2)\n",
    "        if distance > max_distance:\n",
    "            most_similar_words = (word1, word2)\n",
    "            max_distance = distance\n",
    "        if distance < min_distance:\n",
    "            most_distant_words = (word1, word2)\n",
    "            min_distance = distance\n",
    "\n",
    "# Print the most distant and the closest words and their distances\n",
    "print(f\"The most distant words are {most_distant_words} with a distance of {max_distance}\")\n",
    "print(f\"The closest words are {most_similar_words} with a distance of {min_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Using NER (Named Entity Recognition) find the named entities in the text. Print the first 5 most similar words to them both in upper and lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entity: London\n",
      "lowercase: birmingham\n",
      "uppercase: EURASIAN_NATURAL_RESOURCES_CORP.\n",
      "lowercase: nyc\n",
      "uppercase: Londons\n",
      "lowercase: england\n",
      "uppercase: Islamabad_Slyvia_Hui\n",
      "lowercase: manchester\n",
      "uppercase: Wandsworth\n",
      "lowercase: brooklyn\n",
      "uppercase: Canary_Wharf\n",
      "\n",
      "Named entity: Belgium\n",
      "lowercase: austria\n",
      "uppercase: Netherlands\n",
      "lowercase: belgian\n",
      "uppercase: Belgian\n",
      "lowercase: serbia\n",
      "uppercase: Belguim\n",
      "lowercase: italia\n",
      "uppercase: France\n",
      "lowercase: har_en\n",
      "uppercase: writers_Constant_Brand\n",
      "\n",
      "Named entity: Theo\n",
      "lowercase: religio\n",
      "uppercase: bomb_sniffing_springer_spaniel\n",
      "lowercase: allan\n",
      "uppercase: Marcel\n",
      "lowercase: christ\n",
      "uppercase: Stefan\n",
      "lowercase: extrem\n",
      "uppercase: Willem\n",
      "lowercase: francis\n",
      "uppercase: Robbie\n",
      "\n",
      "Named entity: two\n",
      "lowercase: three\n",
      "uppercase: three\n",
      "lowercase: four\n",
      "uppercase: four\n",
      "lowercase: five\n",
      "uppercase: five\n",
      "lowercase: six\n",
      "uppercase: six\n",
      "lowercase: seven\n",
      "uppercase: seven\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.text in model.key_to_index:\n",
    "        print(f\"Named entity: {ent.text}\")\n",
    "        similar_words_lower = model.most_similar(ent.text.lower(), topn=5)\n",
    "        similar_words_upper = model.most_similar(ent.text, topn=5)\n",
    "        for (word_lower, _), (word_upper, _) in zip(similar_words_lower, similar_words_upper):\n",
    "            print(f\"lowercase: {word_lower}\\nuppercase: {word_upper}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Print the clusters of words that are the most similar in the text (you can use sklearn's Kmeans) based on their vectors in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['solitude', 'time', 'spent', 'London', 'man', 'trends', 'signs']\n",
      "Cluster 1: ['while', '1881', 'Gogh', 'financially', 'into', 'Theo', 'maintained', 'his', 'travelling', 'parents', 'was', 'drifted', 'serious', 'as', 'two', 'but', 'back', 'often', 'to', 'aware', 'young', 'after', '.', 'depressed', 'missionary', 'with', 'brother', 'He', 'family', 'him', 'in', 'His', 'and', 'upper-middle-class', 'quiet', 'thoughtful', 'became', 'transferred', 'dealer', 'religion', 'turned', 'worked', 'painting', 'took', 'a', 'southern', 'the', 'drew', 'of', ',', 'supported', 'up', 'keenly', 'Van', 'them', 'an', 'ill-health', 'Belgium', 'he', 'instability']\n",
      "Cluster 2: ['Born']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "vectors = [model[word] for word in words if word in vocab]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(vectors)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "clusters = {i: [] for i in range(kmeans.n_clusters)}\n",
    "\n",
    "for word, label in zip(words, labels):\n",
    "    clusters[label].append(word)\n",
    "\n",
    "for label, words in clusters.items():\n",
    "    print(f\"Cluster {label}: {words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
